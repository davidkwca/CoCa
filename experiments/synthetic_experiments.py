import os, errno
import argparse
import subprocess

import numpy as np
import pandas as pd

import sys
sys.path.append("..")
from models import compare_models
from config import DATA_PATH, CODE_PATH


def data_files(N=500,
               DX=1,
               DZ=1,
               llim=0,
               ulim=50,
               dist_label='n',
               structure='cf',
               data='dr'):
    """Returns a list of (index, filename) which can be used nicely in main."""
    files = []
    for ns in range(llim, ulim):
        fdir = f'{DATA_PATH}/synthetic/{data}/{dist_label}/{structure}/'
        f = fdir + f'n{N}_dx{DX}_dz{DZ}[{ns}].csv'
        files.append((ns, f))
    return files


def main(N,
         DX,
         DZ_true,
         DZ_inferred=1,
         dist_label='n',
         structure='',
         data='dr',
         batch=10):
    """The function which runs the synthetic experiments. Will load data from the data files generated by data_generation.py and process `batch` many of them at once due to problems with pymc models not being removed from memory after they are no longer being used.
    Inputs:
    N: The input size of the data.
    DX: The number of features in X.
    DZ_true: The dimensionality in the confounder generating the data. Used only for the confounded data.
    DZ_inferred: Dimensionality of latent variable inferred. Experiments always used 1.
    dist_label: Which distribution's data to use.
    structure: Causal (cs) or confounded (cf) data?
    data: Data used for decision rate plots (dr) or heatmap plot (heat)?
    batch: How many data sets to run before stopping."""

    assert structure
    # Directory and file name where the results will be stored
    result_dir = f'results/synth/{data}/{structure}'
    os.makedirs(result_dir, exist_ok=True)

    result_fname = '{}_n{}_dx{}_dz{}.csv'.format(dist_label, N, DX, DZ_true)
    result_fname = os.path.join(result_dir, result_fname)

    # If we have already run previous batches, we want to run
    # on new data, so we check which was run last.
    try:
        df = pd.read_csv(result_fname, header=None)
        llim = df.iloc[:, 0].max() + 1
    except FileNotFoundError:
        llim = 0
    ulim = llim + batch

    # List of pairs containing the number of the dataset and its full path.
    files = data_files(
        N, DX, DZ_true, llim, ulim, dist_label, structure=structure, data=data)

    for i, f in files:
        try:
            XY = np.loadtxt(f, delimiter=',')
            X = XY[:, :-1]
            Y = XY[:, -1]
            cs_mean, cf_mean = compare_models(X, Y, DZ_inferred)

            try:
                spectral_beta = subprocess.check_output(
                    f"/usr/bin/Rscript --vanilla {CODE_PATH}experiments/spectral.R {f}",
                    shell=True).decode('utf-8')
                spectral_beta = float(spectral_beta)
            except subprocess.CalledProcessError:
                spectral_beta = np.NaN

            try:
                indep_bp = subprocess.check_output(
                    f"/usr/bin/Rscript --vanilla {CODE_PATH}experiments/indep.R {f}",
                    shell=True).decode('utf-8')
                indep_beta, indep_pval = indep_bp.split(',')
                indep_beta = float(indep_beta)
                indep_pval = float(indep_beta)
            except subprocess.CalledProcessError:
                indep_beta = np.NaN
                indep_pval = np.NaN

            with open(result_fname, 'a+') as f:
                f.write('{},{:.2f},{:.2f},{:.2f},{:.2f},{:.2f}\n'.format(
                    i, cs_mean, cf_mean, spectral_beta, indep_beta,
                    indep_pval))
        # In the experiment setup, files not being found
        # generally just means that we ran too many rounds.
        except FileNotFoundError:
            print(f'File {f} not found.')


if __name__ == '__main__':
    # All these arguments just correspond to the parameters of
    # the main function.
    parser = argparse.ArgumentParser()
    parser.add_argument('-n', type=int, default=500)
    parser.add_argument('-dx', type=int, default=1)
    parser.add_argument('-dz', type=int, default=1)
    parser.add_argument('-dz_inf', type=int, default=1)
    parser.add_argument('-label', type=str, default='n')
    parser.add_argument('-data', type=str, default='dr')
    parser.add_argument('-batch', type=int, default=10)

    args = parser.parse_args()

    # We want to run the method for both confounded and causal data.
    for structure in ['cf', 'cs']:
        main(args.n, args.dx, args.dz, args.dz_inf, args.label, structure,
             args.data, args.batch)
